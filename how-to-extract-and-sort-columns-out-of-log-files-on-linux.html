<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=description content="Sorting a log file by a specific column is useful for finding information quickly. Logs are usually stored as plaintext, so you can use command line text manipulation tools to process them and view them in a more readable manner."><meta name=author content="Reinaldo Massengill"><meta name=generator content="Hugo 0.98.0"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=robots content="index,follow,noarchive"><link rel=stylesheet href=https://assets.cdnweb.info/hugo/base16/css/style.css type=text/css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700" type=text/css><link rel=alternate href=./index.xml type=application/rss+xml title=VibeX><title>How to Extract and Sort Columns Out of Log Files on Linux - VibeX</title></head><body><header><div class="container clearfix"><a class=path href=./index.html>[VibeX]</a>
<span class=caret># _</span><div class=right></div></div></header><div class=container><main role=main class=article><article class=single itemscope itemtype=http://schema.org/BlogPosting><div class=meta><span class=key>published on</span>
<span class=val><time itemprop=datePublished datetime=2024-05-29>May 29, 2024</time></span>
<span class=key>in</span>
<span class=val><a href=./categories/blog>blog</a></span></div><h1 class=headline itemprop=headline>How to Extract and Sort Columns Out of Log Files on Linux</h1><section class=body itemprop=articleBody><h3>Quick Links</h3><ul class=table-content-level-1><li><a href=#>Extracting Columns with cut and awk</a></li></ul><ul class=table-content-level-1><li><a href=#>Sorting Columns with sort and uniq</a></li></ul><ul class=table-content-level-1><li><a href=#>Filtering Data with grep and awk</a></li></ul><ul class=table-content-level-1><li><a href=#>GUI Options for Web Logs</a></li></ul><p>Sorting a log file by a specific column is useful for finding information quickly. Logs are usually stored as plaintext, so you can use command line text manipulation tools to process them and view them in a more readable manner.</p><strong class="an-zone-tag-top ad-zone-advertising-tag"></strong> <strong class="an-zone-tag-bottom ad-zone-advertising-sub-tag"></strong><h2 id=extracting-columns-with-cut-and-awk>Extracting Columns with cut and awk</h2><p>The <code>cut</code>&nbsp;and <code>awk</code>&nbsp;utilities are two different ways to extract a column of information from text files. Both assume your log files are whitespace delimited, for example:</p><pre>column column column</pre><p>This presents an issue if the data within the columns contains whitespaces, such as dates ("Wed Jun 12"). While <code>cut</code>&nbsp;may see this as three separate columns, you can still extract all three of them at once, presuming the structure of your log file is consistent.</p><p><code>cut</code>&nbsp;is very simple to use:</p><pre>cat system.log | cut -d ' ' -f 1-6</pre><p>The <code>cat</code>&nbsp;command reads the contents of <code>system.log</code>&nbsp;and pipes it to <code>cut</code>. The <code>-d</code>&nbsp;flag specifies the delimiter, in this case being a whitespace. (The default is tab, <code>t</code>.) The <code>-f</code>&nbsp;flag specifies which fields to output. This command specifically will print the first six columns of <code>system.log</code>. If you wanted to only print the third column, you'd use the&nbsp;<code>-f 3</code>&nbsp;flag.</p><strong class="an-zone-tag-top ad-zone-advertising-tag"></strong> <strong class="an-zone-tag-bottom ad-zone-advertising-sub-tag"></strong><p><code>awk</code>&nbsp;is more powerful but not as concise.&nbsp;<code>cut</code>&nbsp;is useful for extracting columns, like if you wanted to get a list of IP addresses from your Apache logs. <code>awk</code>&nbsp;can rearrange entire lines, which can be useful for sorting an entire document by a specific column.&nbsp;&nbsp;<code>awk</code>&nbsp;is a full programming language, but you can use a simple command to print columns:</p><pre>cat system.log | awk '{print $1, $2}'</pre><p><code>awk</code>&nbsp;runs your command for each line in the file. By default, it splits the file up by whitespaces and stores each column in variables <code>$1</code>, <code>$2</code>, <code>$3</code>, and so on. By using the <code>print $1</code>&nbsp;command, you can print the first column, but there's no easy way to print a range of columns without using loops.</p><p>One benefit of <code>awk</code>&nbsp;is that the command can reference the whole line at once. The contents of the line are stored in variable <code>$0</code>, which you can use to print the whole line. So, you could, for example, print the third column before printing the rest of the line:</p><strong class="an-zone-tag-top ad-zone-advertising-tag"></strong> <strong class="an-zone-tag-bottom ad-zone-advertising-sub-tag"></strong><pre>awk '{print $3 " " $0}'</pre><p>The <code>" "</code>&nbsp;prints a space between <code>$3</code>&nbsp;and <code>$0</code>. This command repeats column three twice, but you can work around that by setting the <code>$3</code>&nbsp;variable to null:</p><pre>awk '{printf $3; $3=""; print " " $0}'</pre><p>The <code>printf</code>&nbsp;command does not print a new line. Similarly, you can exclude specific columns from the output by setting them all to empty strings before printing <code>$0</code>:</p><pre>awk '{$1=$2=$3=""; print $0}'</pre><p>You can do a lot more with <code>awk</code>, including <a href=#>regex matching</a>, but the out-of-the-box column extraction works well for this use case.</p><h2 id=sorting-columns-with-sort-and-uniq>Sorting Columns with sort and uniq</h2><p>The <code>sort</code>&nbsp;command can be used to order a list of data based on a specific column. The syntax is:</p><pre>sort -k 1</pre><p>where the <code>-k</code>&nbsp;flag denotes the column number. You pipe input into this command, and it spits out an ordered list. By default, <code>sort</code>&nbsp;uses alphabetical order but supports more options through flags, such as <code>-n</code>&nbsp;for numerical sort, <code>-h</code>&nbsp;for suffix sort (1M > 1K), <code>-M</code>&nbsp;for sorting month abbreviations, and <code>-V</code>&nbsp;for sorting file version numbers (file-1.2.3 > file-1.2.1).</p><strong class="an-zone-tag-top ad-zone-advertising-tag"></strong> <strong class="an-zone-tag-bottom ad-zone-advertising-sub-tag"></strong><p>The <code>uniq</code>&nbsp;command filters out duplicate lines, leaving only unique ones. It only works for adjacent lines (for performance reasons), so you'll need to always use it after <code>sort</code>&nbsp;to remove duplicates throughout the file. The syntax is simply:</p><pre>sort -k 1 | uniq</pre><p>If you'd like to only list the duplicates, use the <code>-d</code>&nbsp;flag.</p><p><code>uniq</code>&nbsp;can also count the number of duplicates with the <code>-c</code>&nbsp;flag, which makes it very good for tracking frequency. For example, if you wanted to get a list of the top IP addresses hitting your Apache server, you could run the following command on your <code>access.log</code>:</p><pre>cut -d ' ' -f 1 | sort | uniq -c | sort -nr | head</pre><p>This string of commands will cut out the IP address column, group the duplicates together, remove the duplicates while counting each occurence, then sort based on the count column in descending numerical order, leaving you with a list that looks like:</p><pre>21 192.168.1.1  <p>12 10.0.0.1</p> <strong class="an-zone-tag-top ad-zone-advertising-tag"></strong>  <strong class="an-zone-tag-bottom ad-zone-advertising-sub-tag"></strong>  <p>5 1.1.1.1</p>  <p>2 8.0.0.8</p></pre><p>You can apply these same techniques to your log files, in addition to other utilities like <code>awk</code>&nbsp;and <code>sed</code>, to pull useful information out. These chained commands are long, but you don't have to type them in every time, as you can always store them in a bash script or alias them through your <code>~/.bashrc</code>.</p><h2 id=filtering-data-with-grep-and-awk>Filtering Data with grep and awk</h2><p><code>grep</code>&nbsp;is a very simple command; you give it a search term and pass it input, and it will spit out every line containing that search term. For example, if you wanted to search your Apache access log for 404 errors, you could do:</p><pre>cat access.log | grep "404"</pre><p>which would spit out a list of log entries matching the given text.</p><p>However, <code>grep</code>&nbsp;can't limit its search to a specific column, so this command will fail if you have the text "404" anywhere else in the file. If you want to only search the HTTP status code column, you'll need to use <code>awk</code>:</p><pre>cat access.log | awk '{if ($9 == "404") print $0;}'</pre><strong class="an-zone-tag-top ad-zone-advertising-tag"></strong> <strong class="an-zone-tag-bottom ad-zone-advertising-sub-tag"></strong><p>With <code>awk</code>, you also have the benefit of being able to do negative searches. For example, you can search for all log entries that didn't return with status code 200 (OK):</p><pre>cat access.log | awk '{if ($9 != "200") print $0;}'</pre><p>as well as having access to all the programmatic features <code>awk</code>&nbsp;provides.</p><h2 id=gui-options-for-web-logs>GUI Options for Web Logs</h2><img style=margin:auto;display:block;text-align:center;max-width:100%;height:auto src=https://cdn.statically.io/img/static1.howtogeekimages.com/wordpress/wp-content/uploads/csit/2019/06/0d5f8f09.png><p><a href=#>GoAccess</a> is a CLI utility for monitoring your web server's access log in real time, and sorts by each useful field. It runs entirely in your terminal, so you can use it over SSH, but it also has a much more intuitive web interface.</p><p><a href=#><code>apachetop</code></a>&nbsp;is another utility specifically for apache, that can be used to filter and sort by columns in your access log. It runs in real time directly on your access.log.</p><strong class="an-zone-tag-top ad-zone-advertising-tag"></strong> <strong class="an-zone-tag-bottom ad-zone-advertising-sub-tag"></strong><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7qbvWraagnZWge6S7zGibnq6fpcBwtM6wZK2nXZrFtb7AnKtmmZ6ZerS70a1knKecqrqvv4yorK1ln5t6rbvGZp2ipJWoerC6jKWgp62oZA%3D%3D</p></section></article></main></div><footer><div class=container><span class=copyright>&copy; 2024 VibeX - <a rel=license href=http://creativecommons.org/licenses/by/4.0/>CC BY 4.0</a></span></div></footer><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>